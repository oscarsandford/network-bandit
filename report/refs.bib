%% References here. No fewer than 10 references.

@inbook{mab_algos,
  author    = {Vermorel,Joannès and Mohri,Mehryar},
  year      = {2005},
  title     = {Multi-armed Bandit Algorithms and Empirical Evaluation},
  series    = {Machine Learning: ECML 2005},
  publisher = {Springer Berlin Heidelberg},
  address   = {Berlin, Heidelberg},
  edition   = {1ère éd},
  pages     = {437-448},
  keywords  = {Applied sciences; Artificial intelligence; Bandit Problem; Computer science; control theory; systems; Content Distribution Network; Empirical Evaluation; Exact sciences and technology; Greedy Strategy; Learning and adaptive systems; Reward Distribution},
  isbn      = {0302-9743},
  language  = {English}
}

@article{qos_selection_mab,
  author  = {Modi, Navikkumar and Mary, Philippe and Moy, Christophe},
  journal = {IEEE Transactions on Cognitive Communications and Networking},
  title   = {QoS Driven Channel Selection Algorithm for Cognitive Radio Network: Multi-User Multi-Armed Bandit Approach},
  year    = {2017},
  volume  = {3},
  number  = {1},
  pages   = {49-66},
  doi     = {10.1109/TCCN.2017.2675901}
}

@article{muMAB_wireless,
  author         = {Boldrini, Stefano and De Nardis, Luca and Caso, Giuseppe and Le, Mai T. P. and Fiorina, Jocelyn and Di Benedetto, Maria-Gabriella},
  title          = {muMAB: A Multi-Armed Bandit Model for Wireless Network Selection},
  journal        = {Algorithms},
  volume         = {11},
  year           = {2018},
  number         = {2},
  article-number = {13},
  url            = {https://www.mdpi.com/1999-4893/11/2/13},
  issn           = {1999-4893},
  abstract       = {Multi-armed bandit (MAB) models are a viable approach to describe the problem of best wireless network selection by a multi-Radio Access Technology (multi-RAT) device, with the goal of maximizing the quality perceived by the final user. The classical MAB model does not allow, however, to properly describe the problem of wireless network selection by a multi-RAT device, in which a device typically performs a set of measurements in order to collect information on available networks, before a selection takes place. The MAB model foresees in fact only one possible action for the player, which is the selection of one among different arms at each time step; existing arm selection algorithms thus mainly differ in the rule according to which a specific arm is selected. This work proposes a new MAB model, named measure-use-MAB (muMAB), aiming at providing a higher flexibility, and thus a better accuracy in describing the network selection problem. The muMAB model extends the classical MAB model in a twofold manner; first, it foresees two different actions: to measure and to use; second, it allows actions to span over multiple time steps. Two new algorithms designed to take advantage of the higher flexibility provided by the muMAB model are also introduced. The first one, referred to as measure-use-UCB1 (muUCB1) is derived from the well known UCB1 algorithm, while the second one, referred to as Measure with Logarithmic Interval (MLI), is appositely designed for the new model so to take advantage of the new measure action, while aggressively using the best arm. The new algorithms are compared against existing ones from the literature in the context of the muMAB model, by means of computer simulations using both synthetic and captured data. Results show that the performance of the algorithms heavily depends on the Probability Density Function (PDF) of the reward received on each arm, with different algorithms leading to the best performance depending on the PDF. Results highlight, however, that as the ratio between the time required for using an arm and the time required to measure increases, the proposed algorithms guarantee the best performance, with muUCB1 emerging as the best candidate when the arms are characterized by similar mean rewards, and MLI prevailing when an arm is significantly more rewarding than others. This calls thus for the introduction of an adaptive approach capable of adjusting the behavior of the algorithm or of switching algorithm altogether, depending on the acquired knowledge on the PDF of the reward on each arm.},
  doi            = {10.3390/a11020013}
}

@article{multiuser_mab,
  author  = {Avner, Orly and Mannor, Shie},
  journal = {IEEE/ACM Transactions on Networking},
  title   = {Multi-User Communication Networks: A Coordinated Multi-Armed Bandit Approach},
  year    = {2019},
  volume  = {27},
  number  = {6},
  pages   = {2192-2207},
  doi     = {10.1109/TNET.2019.2935043}
}

@article{mab_wireless_scheduling_survey,
  author  = {Li, Feng and Yu, Dongxiao and Yang, Huan and Yu, Jiguo and Karl, Holger and Cheng, Xiuzhen},
  journal = {IEEE Wireless Communications},
  title   = {Multi-Armed-Bandit-Based Spectrum Scheduling Algorithms in Wireless Networks: A Survey},
  year    = {2020},
  volume  = {27},
  number  = {1},
  pages   = {24-30},
  doi     = {10.1109/MWC.001.1900280}
}
